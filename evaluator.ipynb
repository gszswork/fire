{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b29023",
   "metadata": {},
   "source": [
    "The pipeline to generate all the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d0f710a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model: openai:gpt-4o-mini\n",
      "Loading OpenAI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All fact chekcing results saved to file: results/fire_factcheckbench_gpt-4o-mini.jsonl\n",
      "Failed claims: 0\n",
      "Tokens Used: 488\n",
      "\tPrompt Tokens: 262\n",
      "\tCompletion Tokens: 226\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0001749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import dataclasses\n",
    "import tqdm\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "from common.modeling import Model\n",
    "from common.shared_config import openai_api_key, serper_api_key, anthropic_api_key\n",
    "from common.utils import calculate_cost_claude\n",
    "\n",
    "# Set API keys for various services\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_api_key\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "os.environ[\"SERPER_API_KEY\"] = serper_api_key\n",
    "\n",
    "# List of models to use for the benchmark\n",
    "models = ['openai:gpt-4o-mini']  # Can include other models like Claude, Llama, Mistral, etc.\n",
    "# ['gpt-4o-mini', 'gpt-4o', 'o1-preview', 'o1-mini', 'claude-3-haiku-20240307', 'claude-3-opus-20240229', 'claude-3-5-sonnet-20240620', 'Meta-Llama-3.1-8B-Instruct', 'Mistral-7B-Instruct-v0.3',]\n",
    "# Define benchmark dataset and framework type\n",
    "benchmark = 'factcheckbench'  # e.g., 'bingcheck', 'factool_qa', etc.\n",
    "framework = 'fire'  # Could be 'safe' or 'fire', depending on the task\n",
    "\n",
    "# Import the appropriate fact-checking function based on the framework\n",
    "if framework == 'fire':\n",
    "    from eval.fire.verify_atomic_claim import verify_atomic_claim\n",
    "# elif framework == 'safe':\n",
    "#     from eval.safe.rate_atomic_fact import check_atomic_fact\n",
    "\n",
    "for model in models:\n",
    "    with get_openai_callback() as cb:\n",
    "        print(f'Running model: {model}')\n",
    "        rater = Model(model)\n",
    "        failed_cnt = 0\n",
    "        model_name = model.split(':')[-1].split('/')[-1]  # Extract model name for file saving\n",
    "\n",
    "        # Initialize total token usage tracking\n",
    "        total_usage = {\n",
    "            'input_tokens': 0,\n",
    "            'output_tokens': 0,\n",
    "        }\n",
    "\n",
    "        # Open output file to save the results\n",
    "        with open(f'results/{framework}_{benchmark}_{model_name}.jsonl', 'w') as fout:\n",
    "            # Read and process each line from the dataset\n",
    "            for line in tqdm.tqdm(open(f'datasets/{benchmark}/data.jsonl', 'r').readlines()[:1]):\n",
    "                data = json.loads(line)  # Load JSON data\n",
    "                claim = data['claim']    # Extract the claim\n",
    "                label = data['label']    # Extract the label\n",
    "\n",
    "                # Run the fact-checking function for the claim\n",
    "                result, searches, usage = verify_atomic_claim(claim, rater)\n",
    "\n",
    "                # Track token usage if available\n",
    "                if usage is not None:\n",
    "                    total_usage['input_tokens'] += usage['input_tokens']\n",
    "                    total_usage['output_tokens'] += usage['output_tokens']\n",
    "\n",
    "                # If result is None, count it as a failure and skip further processing\n",
    "                if result is None:\n",
    "                    failed_cnt += 1\n",
    "                    continue\n",
    "\n",
    "                # Write the result to the output file\n",
    "                fout.write(json.dumps({\n",
    "                    'claim': claim,\n",
    "                    'label': label,\n",
    "                    'result': dataclasses.asdict(result),\n",
    "                    'searches': searches\n",
    "                }) + '\\n')\n",
    "            \n",
    "        print(f'All fact chekcing results saved to file: results/{framework}_{benchmark}_{model_name}.jsonl')\n",
    "        # Print the count of failed claims and usage callback\n",
    "        print(f'Failed claims: {failed_cnt}')\n",
    "        print(cb)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
